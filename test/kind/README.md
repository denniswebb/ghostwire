# KIND Integration Testing

This directory provides a repeatable workflow for exercising the `ghostwire init` command against a live Kubernetes API using a local [KIND](https://kind.sigs.k8s.io/) cluster. It complements the unit tests in `internal/` by validating service discovery, DNAT rule wiring, and generated audit artefacts end to end.

## Prerequisites

Install the following tools before running the scripts:

| Tool | Purpose | Install |
| ---- | ------- | ------- |
| [KIND](https://kind.sigs.k8s.io/) | Local Kubernetes cluster in Docker | `brew install kind` |
| [kubectl](https://kubernetes.io/docs/tasks/tools/) | Kubernetes CLI | `brew install kubernetes-cli` |
| [Docker](https://docs.docker.com/get-docker/) | Image build/runtime for KIND nodes | Desktop app or `brew install --cask docker` |
| [mise](https://mise.jdx.dev/) (optional) | Project task runner | `brew install mise` |

> KIND requires Docker Desktop (or another container runtime) to be running.

## Quick Start

```sh
# 1. Create the cluster
./test/kind/setup-cluster.sh

# 2. Build the CLI, create a container image, and push it to KIND
./test/kind/load-image.sh

# 3. Deploy namespace, RBAC, and service fixtures
./test/kind/deploy-test.sh

# 4. (Optional) Deploy the ghostwire init pod immediately
./test/kind/deploy-test.sh --with-pod

# 5. Validate generated artefacts
./test/kind/validate-dnatmap.sh
./test/kind/validate-iptables.sh

# 6. (Optional) Deploy and exercise the watcher sidecar (see Watcher Testing below)
./test/kind/deploy-watcher-test.sh
./test/kind/test-watcher-transitions.sh
```

Finish with `./test/kind/teardown-cluster.sh` to reclaim resources.

## Directory Layout

| Path | Description |
| ---- | ----------- |
| `cluster-config.yaml` | KIND configuration for a single control-plane node with port 8081 exposed for watcher metrics. |
| `setup-cluster.sh` / `teardown-cluster.sh` | Scripts to create and delete the cluster. |
| `load-image.sh` | Builds the ghostwire binary, packages it into a Docker image, and loads it into the KIND node. |
| `deploy-test.sh` | Applies namespace/RBAC/service fixtures and optionally launches the init test pod. |
| `deploy-watcher-test.sh` | Applies watcher RBAC and launches the watcher test pod. |
| `validate-dnatmap.sh` | Copies `/shared/dnat.map` from the init pod and checks expected mappings. |
| `validate-iptables.sh` | Executes `iptables -t nat -S CANARY_DNAT` inside the test pod's debug container to verify DNAT rules. |
| `test-watcher-transitions.sh` | Automates watcher label transitions, metrics checks, and iptables validation. |
| `manifests/` | Kubernetes manifests used by the integration environment (namespace, services, RBAC, and test pod). |
| `manifests/watcher-rbac.yaml` | ServiceAccount, Role, and RoleBinding granting the watcher pod access to its labels. |
| `manifests/watcher-test-pod.yaml` | Pod manifest with init + watcher containers and shared volume for DNAT maps. |

## Manual Testing Procedure

1. **Cluster setup**
   ```sh
   ./test/kind/setup-cluster.sh
   kind get clusters
   kubectl cluster-info --context kind-ghostwire-test
   ```
2. **Apply fixtures**
   ```sh
   ./test/kind/deploy-test.sh
   kubectl --context kind-ghostwire-test get svc -n ghostwire-test
   ```
3. **Build and load image**
   ```sh
   ./test/kind/load-image.sh
   kind load docker-image ghostwire:local --name ghostwire-test  # no-op if already run
   ```
4. **Run the init pod**
   ```sh
   ./test/kind/deploy-test.sh --with-pod
   kubectl --context kind-ghostwire-test logs -n ghostwire-test ghostwire-init-test
   ```
5. **Inspect artefacts**
   ```sh
   kubectl --context kind-ghostwire-test exec -n ghostwire-test ghostwire-init-test -- cat /shared/dnat.map
   kubectl --context kind-ghostwire-test exec -n ghostwire-test ghostwire-init-test -c debug -- iptables -t nat -L CANARY_DNAT -n -v
   ```

## Validation Scripts

- `validate-dnatmap.sh`: copies `/shared/dnat.map` from the init pod, prints its contents, and checks for expected entries (orders, payment, api-v2) while ensuring services without preview variants are absent.
- `validate-iptables.sh`: retrieves active and preview cluster IPs via `kubectl`, then verifies that the init test pod's `CANARY_DNAT` chain contains matching DNAT rules and the metadata service exclusion from inside the debug container.
- `test-watcher-transitions.sh`: changes the watcher pod's role label, confirms jump rule activation/removal, and inspects `/metrics` and `/healthz` outputs (detailed in the Watcher Testing section below).

Both scripts exit non-zero on failure so they can be chained into automated smoke tests.

## Expected Results

`/shared/dnat.map` should resemble:

```
# DNAT mappings generated by ghostwire-init
# Format: service:port/protocol active_ip -> preview_ip
orders:80/TCP 10.96.x.y -> 10.96.x.z
orders:443/TCP 10.96.x.y -> 10.96.x.z
payment:8080/TCP 10.96.x.a -> 10.96.x.b
api-v2:8080/TCP ...
api-v2:8443/TCP ...
api-v2:9090/TCP ...
```

`iptables -t nat -S CANARY_DNAT` should include:

- `-A CANARY_DNAT -d 169.254.169.254/32 -j RETURN` (IMDS exclusion).
- One DNAT rule per mapping listed above.

## Watcher Testing

### Overview

The watcher sidecar polls the pod's `role` label, toggles the `CANARY_DNAT` jump rule, and exposes `/metrics` and `/healthz` on `:8081`. These tests build on the init workflow: the init container primed the DNAT map and verified service discovery; the watcher validates ongoing routing control using the same fixtures.

### Prerequisites

- Complete the **Quick Start** steps above, including `deploy-test.sh --with-pod`, so the DNAT map exists on the shared volume.
- Load the `ghostwire:local` image into KIND via `./test/kind/load-image.sh` (required for both init and watcher containers).
- Ensure the cluster is running (`kind get clusters`) and the `ghostwire-test` namespace exists.

### Quick Start

```sh
# Deploy watcher RBAC + pod
./test/kind/deploy-watcher-test.sh

# Run automated transition checks
./test/kind/test-watcher-transitions.sh

# Follow logs while experimenting manually
kubectl --context kind-ghostwire-test logs -n ghostwire-test ghostwire-watcher-test -c ghostwire-watcher -f
```

### Manual Testing Procedure

1. Deploy RBAC and the watcher pod: `./test/kind/deploy-watcher-test.sh`.
2. Confirm the pod is running and initially labelled `role=active`: `kubectl get pod ghostwire-watcher-test -n ghostwire-test --show-labels`.
3. Verify the jump rule is absent in active mode: `kubectl exec ... -- iptables -t nat -L OUTPUT -n -v` (no `CANARY_DNAT` entry).
4. Switch to preview mode: `kubectl label pod ghostwire-watcher-test -n ghostwire-test role=preview --overwrite` and wait a few seconds.
5. Check logs for `activating dnat jump` and confirm the jump rule now exists at position 1 in `OUTPUT`.
6. Inspect observability endpoints:
   ```sh
   kubectl exec ... -- wget -qO- http://localhost:8081/healthz
   kubectl exec ... -- wget -qO- http://localhost:8081/metrics | grep ghostwire_jump_active
   ```
7. Revert to active mode with `kubectl label ... role=active --overwrite` and verify the jump rule is removed and logs show `deactivating dnat jump`.

### Expected Results

- **Logs:** transitions log at `INFO` level (`activating dnat jump`, `deactivating dnat jump`) while unmatched values log at `DEBUG`.
- **iptables:** `iptables -t nat -S OUTPUT` contains `-I OUTPUT 1 -j CANARY_DNAT` only when `role=preview`.
- **Metrics (sample):**

  ```
  # HELP ghostwire_jump_active Whether the DNAT jump rule is active (1) or inactive (0).
  # TYPE ghostwire_jump_active gauge
  ghostwire_jump_active 0
  # HELP ghostwire_errors_total Total number of watcher errors by type.
  # TYPE ghostwire_errors_total counter
  # HELP ghostwire_dnat_rules Number of DNAT rules discovered from the audit map.
  # TYPE ghostwire_dnat_rules gauge
  ghostwire_dnat_rules <count derived from /shared/dnat.map>
  ```
- **Health:** `/healthz` returns `200 OK` with body `OK` once the chain has been verified and labels have been read at least once.

### Troubleshooting

- **Watcher pod stays Pending:** confirm `deploy-test.sh --with-pod` was run so the shared volume and DNAT map exist, and ensure `deploy-watcher-test.sh` applied the ServiceAccount/Role.
- **Health check returns 503:** the watcher has not yet verified the DNAT chain (`SetChainVerified`) or read labels (`SetLabelsRead`); inspect logs for warnings and ensure the init container completed successfully.
- **Jump rule not added:** verify the pod has `NET_ADMIN` capability, confirm label changes took effect (`kubectl get pod ... --show-labels`), and review watcher logs for iptables errors.
- **Metrics missing or empty:** use `kubectl exec` to curl `/metrics`; if the endpoint is unreachable, ensure port 8081 is exposed and the pod is running.

### Validation Scripts

`./test/kind/test-watcher-transitions.sh` automates the manual flow: it resets the label to `active`, toggles to `preview`, validates jump rule insertion/removal, scrapes `/metrics` for expected gauges/counters, and checks `/healthz`. The script exits non-zero on the first failure so it can be chained into CI smoke runs. Re-run `./test/kind/deploy-watcher-test.sh` if the pod has been deleted between runs.

## Troubleshooting

- **`kind: command not found`** – Install KIND and ensure Docker Desktop is running.
- **Cluster already exists** – `setup-cluster.sh` will prompt to recreate. Answer `y` to rebuild from scratch.
- **`load-image.sh` fails** – Ensure Docker is running and you have built the binary (`mise run build`).
- **Pod stuck in `ImagePullBackOff`** – Re-run `load-image.sh`; KIND nodes do not pull from Docker Desktop automatically.
- **`validate-dnatmap.sh` copy failure** – The init pod must still be present. Re-run `deploy-test.sh --with-pod` to recreate it.
- **`validate-iptables.sh` missing rules** – Confirm the init pod completed successfully and review pod logs for service discovery warnings.

## Cleanup

```sh
./test/kind/teardown-cluster.sh
rm -f ghostwire
```

## Next Steps

With both the init container and watcher sidecar validated in KIND, the next iteration can focus on scripting these flows into `act` jobs or GitHub Actions smoke tests, and exploring IPv6 toggle scenarios for clusters that enable dual-stack networking.
